{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "from transformers import BertTokenizer\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\" #加载BERT英文预训练模型，不区分大小写\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7aa0958547b46d7b09230d275e3479b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e361af3aa640c192e697e4cd1c0a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb5a3f67b8e4f92a80ffc46cdc61606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc879e0b79df41c88797331c527abecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict size 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "vocab = tokenizer.vocab\n",
    "print(\"dict size\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = TAG_RE.sub('', sen)\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHARLESTON C After three tours in Iraq and Afghanistan J Hardin wound up hiding from the world in backwoods cabin in North Carolina Divorced alcoholic and at times suicidal he had tried almost all the accepted treatments for stress disorder psychotherapy group therapy and nearly dozen different medications Nothing worked for me so put aside the idea that could get better said Mr Hardin just pretty much became hermit in my cabin and never went out Then in he joined small drug trial testing whether PTSD could be treated with MDMA the illegal party drug better known as Ecstasy It changed my life he said in recent interview in the bright airy living room of the suburban ranch house here where he now lives while going to college and working as an airplane mechanic It allowed me to see my trauma without fear or hesitation and finally process things and move forward Based on promising results like Mr Hardin the Food and Drug Administration gave permission Tuesday for Phase clinical trials of the drug final step before the possible approval of Ecstasy as prescription drug If successful the trials could turn an illicit street substance into potent treatment for PTSD Through spokeswoman the D declined to comment citing regulations that prohibit disclosing information about drugs that are being developed m cautious but hopeful said Dr Charles Marmar the head of psychiatry at New York University Langone School of Medicine leading PTSD researcher who was not involved in the study If they can keep getting good results it will be of great use PTSD can be very hard to treat Our best therapies right now don help to percent of people So we need more options But he expressed concern about the potential for abuse It a drug and we know people are prone to abuse it he said Prolonged use can lead to serious damage to the brain The Multidisciplinary Association for Psychedelic Studies small nonprofit created in to advocate the legal medical use of MDMA LSD marijuana and other banned drugs sponsored six Phase studies treating total of PTSD patients with the stimulant It will also fund the Phase research which will include at least patients Two trials here in Charleston focused on treating combat veterans sexual assault victims and police and firefighters with PTSD who had not responded to traditional prescription drugs or psychotherapy Patients had on average struggled with symptoms for years After three doses of MDMA administered under psychiatrist guidance the patients reported percent decrease of severity of symptoms on average one study found By the end of the study no longer met the criteria for having PTSD examinations found that improvements lasted more than year after therapy We can sometimes see this kind of remarkable improvement in traditional psychotherapy but it can take years if it happens at all said Dr Michael Mithoefer the psychiatrist who conducted the trials here We think it works as catalyst that speeds the natural healing process The researchers are so optimistic that they have applied for breakthrough therapy status with the Food and Drug Administration which would speed the approval process If approved the drug could be available by Under the researchers proposal for approval the drug would be used limited number of times in the presence of trained psychotherapists as part of broader course of therapy But even in those controlled circumstances some scientists worry that approval as therapy could encourage more illegal recreational use It sends the message that this drug will help you solve your problems when often it just creates problems said Andrew Parrott psychologist at Swansea University in Wales who has studied the brains of chronic Ecstasy users This is messy drug we know can do damage Allowing doctors to administer the drug to treat disorder he warned could inadvertently lead to wave of abuse similar to the current opioid crisis During initial studies patients went through weeks of psychotherapy including three sessions in which they took MDMA During the sessions they lay on futon amid candles and fresh flowers listening to soothing music Dr Mithoefer and his wife Ann Mithoefer and often their portly terrier mix Flynn sat with each patient guiding them through traumatic memories The medicine allows them to look at things from different place and reclassify them said Ms Mithoefer psychiatric nurse Honestly we don have to do much Each person has an innate ability to heal We just create the right conditions Research has shown that the drug causes the brain to release flood of hormones and neurotransmitters that evoke feelings of trust love and while also muting fear and negative emotional memories that can be overpowering in patients with stress disorder Patients say the drug gave them heightened clarity and ability to address their problems For years after his combat deployments Mr Hardin said he was sleepless and on edge His dreams were marked with explosions and death The Army gave him sleeping pills and antidepressants When they didn work he turned to alcohol and began withdrawing from the world just felt hopeless and in the dark he said But the MDMA sessions showed me light could move toward Now m out of the darkness and the world is all around me Since the trial he has gone back to school and remarried The chemist Alexander Shulgin first realized the traits of MDMA in the and introduced it to psychologists he knew Under the nickname Adam thousands of psychologists began to use it as an aid for therapy sessions Some researchers at the time thought the drug could be helpful for anxiety disorders including PTSD but before formal clinical trails could start Adam spread to dance clubs and college campuses under the name Ecstasy and in the Drug Enforcement Administration made it Schedule drug barring all legal use Since then the number of people seeking treatment for PTSD has exploded and psychiatry has struggled to keep pace Two drugs approved for treating the disorder worked only mildly better than placebos in trials Current psychotherapy approaches are often slow and many patients drop out when they don see results Studies have shown combat veterans are particularly hard to treat In interviews study participants said MDMA therapy had not only helped them with painful memories but also had helped them stop abusing alcohol and other drugs and put their lives back together On recent evening Edward Thompson former firefighter tucked his twin girls into bed turned on their night light then joined his wife at backyard fire If it weren for MDMA he said He be dead his wife Laura finished They both nodded Years of responding to gory accidents left Mr Thompson in near constant state of panic that he had tried to numb with alcohol and prescription opiates and benzodiazepines By efforts at therapy had failed and so had several family interventions His wife had left with their children and he was considering jumping in front of bus member of conservative Anglican church Mr Thompson had never used illegal drugs But he was struggling with addiction from his prescription drugs so he at first rejected suggestion by his therapist that he enter the study In the end was out of choices he said Three sessions with the drug gave him the clarity he said to identify his problems and begin to work through them He does not wish to take the drug again It gave me my life back but it wasn a party drug he said It was lot of work ', 0]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "#加载fake news 数据集\n",
    "\"\"\"\n",
    "Load data from news sources.\n",
    "Requires data from https://www.kaggle.com/c/fake-news/data to be saved to \"./train.csv\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# split data to 10 fold\n",
    "fold_num = 10\n",
    "data_file = 'train.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "f = pd.read_csv(data_file, encoding='UTF-8',usecols=['text', 'label'])\n",
    "texts = f['text'].tolist()[:20761]\n",
    "labels = f['label'].tolist()[:20761]\n",
    "index = list(range(20761))\n",
    "np.random.shuffle(index)\n",
    "    \n",
    "all_texts = []\n",
    "all_labels = []\n",
    "data=[]\n",
    "for i in index:\n",
    "    if labels[i] == 1:\n",
    "        data.append([preprocess_text(str(texts[i])), 1])\n",
    "    elif labels[i] == 0:\n",
    "        data.append([preprocess_text(str(texts[i])), 0])\n",
    "\n",
    "print(data[1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size: 19516\n",
      "valset size: 1245\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  \n",
    "        self.mode = mode\n",
    "        self.df = data #its list [['text1',label],['text2',label],...]\n",
    "        self.len = len(self.df)\n",
    "        self.maxlen = 200 \n",
    "        self.tokenizer = tokenizer  # we will use BERT tokenizer\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        origin_text = self.df[idx][0]\n",
    "        if self.mode == \"test\":\n",
    "            text_a = self.df[idx][0]\n",
    "            text_b = None  #for natural language inference\n",
    "            #label_tensor = None #in our case, we have label\n",
    "            label_id = self.df[idx][1]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "        else:     \n",
    "            text_a = self.df[idx][0]\n",
    "            text_b = None  #for natural language inference\n",
    "            label_id = self.df[idx][1]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        \n",
    "       \n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a[:self.maxlen] + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        if text_b is not None:\n",
    "            tokens_b = self.tokenizer.tokenize(text_b)\n",
    "            word_pieces += tokens_b + [\"[SEP]\"]\n",
    "            len_b = len(word_pieces) - len_a\n",
    "               \n",
    "       \n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "       \n",
    "        if text_b is None:\n",
    "            segments_tensor = torch.tensor([1] * len_a,dtype=torch.long)\n",
    "        elif text_b is not None:\n",
    "            segments_tensor = torch.tensor([0] * len_a + [1] * len_b,dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor, origin_text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "# initialize Dataset\n",
    "trainset = MyDataset(\"train\", tokenizer=tokenizer)\n",
    "# testset = MyDataset(\"test\", tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "#split val from trainset\n",
    "val_size = int(trainset.__len__()*0.06) \n",
    "trainset, valset = random_split(trainset,[trainset.__len__()-val_size,val_size])\n",
    "print('trainset size:' ,trainset.__len__())\n",
    "print('valset size:',valset.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:\n",
      " ['[CLS]', 'want', 'to', 'get', 'this', 'briefing', 'by', 'email', 'here', 'the', 'good', 'evening', 'here', 'the', 'latest', 'the', 'meeting', 'between', 'tech', 'executives', 'and', 'donald', 'trump', 'included', 'leaders', 'of', 'some', 'of', 'the', 'world', 'most', 'successful', 'companies', 'including', 'jeff', 'be', '##zos', 'of', 'amazon', 'above', 'three', 'of', 'mr', 'trump', 'children', 'attended', 'what', 'unfolded', 'is', 'less', 'immediately', 'clear', 'the', 'press', 'was', 'quickly', 'ushered', 'out', 'of', 'the', 'room', 'mr', 'be', '##zos', 'had', 'already', 'made', 'news', 'announcing', 'that', 'amazon', 'made', 'its', 'first', 'commercial', 'drone', 'delivery', 'this', 'month', 'in', 'britain', 'deal', 'to', 'end', 'the', 'brutal', 'assaults', 'on', 'the', 'syrian', 'city', 'of', 'aleppo', 'fell', 'through', 'artillery', 'shell', '##ing', 'resumed', 'in', 'the', 'latest', 'bitter', 'turn', 'for', 'the', 'thousands', 'of', 'civilians', 'and', 'medical', 'workers', 'unable', 'to', 'flee', 'the', 'ruined', 'city', 'for', 'territory', 'held', 'by', 'the', 'government', 'nurse', 'said', 'he', 'hoped', 'the', 'world', 'would', 'hear', 'our', 'final', 'scream', 'in', 'washington', 'congressional', 'republicans', 'face', 've', '##xing', 'dilemma', 'over', 'russia', 'the', 'party', 'cat', '##ech', '##ism', 'teaches', 'tough', 'line', 'on', 'what', 'it', 'calls', 'russian', 'advent', '##uri', '##sm', 'and', 'republican', 'leaders', 'have', 'been', 'in', '##fur', '##iated', 'by', 'evidence', 'of', 'russian', 'cyber', '##att', '##ack', '##s', 'affecting', 'the', 'presidential', 'election', 'and', 'even', 'some', 'house', 'races', 'but', 'mr', 'trump', 'who', 'has', 'dismissed', 'the', 'idea', 'that', 'the', 'attacks', 'were', 'in', 'support', 'of', '[SEP]'] \n",
      "\n",
      "origin_text:\n",
      "  Want to get this briefing by email Here the Good evening Here the latest The meeting between tech executives and Donald Trump included leaders of some of the world most successful companies including Jeff Bezos of Amazon above Three of Mr Trump children attended What unfolded is less immediately clear the press was quickly ushered out of the room Mr Bezos had already made news announcing that Amazon made its first commercial drone delivery this month in Britain deal to end the brutal assaults on the Syrian city of Aleppo fell through Artillery shelling resumed in the latest bitter turn for the thousands of civilians and medical workers unable to flee the ruined city for territory held by the government nurse said he hoped the world would hear our final scream In Washington congressional Republicans face vexing dilemma over Russia The party catechism teaches tough line on what it calls Russian adventurism and Republican leaders have been infuriated by evidence of Russian cyberattacks affecting the presidential election and even some House races But Mr Trump who has dismissed the idea that the attacks were in support of his campaign wants votes for his pick for secretary of state Rex Tillerson Exxon Mobil chief executive and longtime friend of the Russian president Vladimir Putin North Carolina bitter election battle is over but the struggle for control of state government is not In surprise special session Republicans in the Legislature moved to strip the incoming Democratic governor Roy Cooper of power over election boards his own cabinet and many state employees It appears this fourth special session will be to nullify the vote of the people the leader of House Democrats said Yahoo disclosed second enormous hack that compromised user accounts The company says that billion users were affected and sensitive information may have been obtained The breach happened in year before hackers stole data from million users The Federal Reserve citing the steady growth of the S economy will increase its benchmark interest rate for just the second time since the financial crisis The move means that all sorts of expenses including the cost of housing cars student loans and even the interest on your credit card will eventually go up The rate close to zero for years remains low by historical standards was really looking for confrontation so could kill That what President Rodrigo Duterte of the Philippines said boasting that he had personally killed criminals during his time as mayor of Davao City He is now overseeing bloody crackdown on drug dealers and users in the country above Mexico is also grappling with bloodshed An increasingly volatile criminal landscape has fueled surge in homicides with cases recorded in the first months of this year The Obama administration issued rule to stop states from withholding federal funds for health clinics that provide abortions Federal law already prohibits government financing for abortions with some exceptions The new rule is meant to prevent states from cutting off funding to Planned Parenthood affiliates and other groups for unrelated health services Separately new study undermines the claim that women who terminate pregnancies experience emotional and psychological trauma In yet another jolt to the auction world the chief executive of Christie has decided to step down Patricia Barbizet above will hand the reins to Guillaume Cerutti The news came just week after Brett Gorvy Christie top announced that he would be leaving Auction houses have been struggling for inventory amid political and economic uncertainty The legions of bots operating on Twitter may be best known for incessantly spewing spam or insults but there an emerging class of politicized bots taking hold You could think of them as protest bots our media critic writes Some fight disinformation with facts and others are meant to distract trolls One is identifying every American who donated to Mr Trump campaign The act of running may not seem to require much thought but new study suggests that it enhances cognition Research shows that for certain cognitive skills like multitasking and concentration the brains of competitive distance runners were more finely honed than those of inactive people Finally Super Mario is coming to our iPhones and iPads The release of Super Mario Run on Thursday means that one of the games in history or at least one based on it can be played on one of the most popular electronic devices in the world It not cheap for full access And to limit piracy it requires full internet connection Photographs may appear out of order for some readers Viewing this version of the briefing should help Your Evening Briefing is posted at m Eastern And don miss Your Morning Briefing posted weekdays at m Eastern and Your Weekend Briefing posted at m Sundays Want to look back Here last night briefing What did you like What do you want to see here Let us know at briefing nytimes com  \n",
      "\n",
      "label: reliable \n",
      "\n",
      "tokens_tensor:\n",
      " tensor([  101,  2215,  2000,  2131,  2023, 27918,  2011, 10373,  2182,  1996,\n",
      "         2204,  3944,  2182,  1996,  6745,  1996,  3116,  2090,  6627, 12706,\n",
      "         1998,  6221,  8398,  2443,  4177,  1997,  2070,  1997,  1996,  2088,\n",
      "         2087,  3144,  3316,  2164,  5076,  2022, 28370,  1997,  9733,  2682,\n",
      "         2093,  1997,  2720,  8398,  2336,  3230,  2054, 23959,  2003,  2625,\n",
      "         3202,  3154,  1996,  2811,  2001,  2855, 23815,  2041,  1997,  1996,\n",
      "         2282,  2720,  2022, 28370,  2018,  2525,  2081,  2739, 13856,  2008,\n",
      "         9733,  2081,  2049,  2034,  3293, 18465,  6959,  2023,  3204,  1999,\n",
      "         3725,  3066,  2000,  2203,  1996, 12077, 22664,  2006,  1996,  9042,\n",
      "         2103,  1997, 22973,  3062,  2083,  4893,  5806,  2075,  7943,  1999,\n",
      "         1996,  6745,  8618,  2735,  2005,  1996,  5190,  1997,  9272,  1998,\n",
      "         2966,  3667,  4039,  2000, 10574,  1996,  9868,  2103,  2005,  3700,\n",
      "         2218,  2011,  1996,  2231,  6821,  2056,  2002,  5113,  1996,  2088,\n",
      "         2052,  2963,  2256,  2345,  6978,  1999,  2899,  7740, 10643,  2227,\n",
      "         2310, 19612, 21883,  2058,  3607,  1996,  2283,  4937, 15937,  2964,\n",
      "        12011,  7823,  2240,  2006,  2054,  2009,  4455,  2845, 13896,  9496,\n",
      "         6491,  1998,  3951,  4177,  2031,  2042,  1999, 27942, 15070,  2011,\n",
      "         3350,  1997,  2845, 16941, 19321,  8684,  2015, 12473,  1996,  4883,\n",
      "         2602,  1998,  2130,  2070,  2160,  3837,  2021,  2720,  8398,  2040,\n",
      "         2038,  7219,  1996,  2801,  2008,  1996,  4491,  2020,  1999,  2490,\n",
      "         1997,   102]) \n",
      "\n",
      "segments_tensor:\n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sample_idx = 10\n",
    "\n",
    "#id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor,origin_text = trainset[sample_idx]\n",
    "\n",
    "# 将 tokens_tensor 还原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "\n",
    "label_map = {0: 'reliable', 1: 'unreliable'}\n",
    "\n",
    "print('token:\\n',tokens,'\\n')\n",
    "print('origin_text:\\n',origin_text,'\\n')\n",
    "print('label:',label_map[int(label_tensor.numpy())],'\\n')\n",
    "print('tokens_tensor:\\n',tokens_tensor,'\\n')\n",
    "print('segments_tensor:\\n',segments_tensor,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\"\"\"\"\n",
    "create_mini_batch(samples)吃上面定義的mydataset\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "#限制attention只注意非pad 的部分\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n",
    "    \n",
    "    # attention masks: 标注不为zero padding的部分\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape,dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "\n",
    "# 初始化DataLoader：设置batch size\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "# testloader = DataLoader(testset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "\n",
    "data = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([16, 202]) \n",
      "tensor([[  101,  6221,  8398,  ...,     0,     0,     0],\n",
      "        [  101,  2281,  3481,  ...,  1997,  1996,   102],\n",
      "        [  101,  2023,  2695,  ...,  2031,  4942,   102],\n",
      "        ...,\n",
      "        [  101,  6643,  1996,  ...,  2014, 11368,   102],\n",
      "        [  101, 21073,  7041,  ...,  2057,  5987,   102],\n",
      "        [  101,  2006,  1996,  ...,  3679,  2003,   102]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([16, 202])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([16, 202])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([16])\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 202])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensors, segments_tensors, \\\n",
    "    masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")\n",
    "tokens_tensors.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name      module\n",
      "--------------------\n",
      "bert      embeddings\n",
      "bert      encoder\n",
      "bert      pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "name      module\n",
    "--------------------\"\"\")\n",
    "\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(\"{:10}{}\".format(name,n) )\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "[epoch 1] loss: 0.1767, acc: 0.9299, val loss: 0.073285, val acc: 0.975160\n",
      "[epoch 2] loss: 0.0638, acc: 0.9773, val loss: 0.066127, val acc: 0.974359\n",
      "[epoch 3] loss: 0.0284, acc: 0.9906, val loss: 0.083928, val acc: 0.974359\n",
      "[epoch 4] loss: 0.0156, acc: 0.9946, val loss: 0.072660, val acc: 0.984776\n",
      "[epoch 5] loss: 0.0141, acc: 0.9950, val loss: 0.077864, val acc: 0.979167\n",
      "[epoch 6] loss: 0.0108, acc: 0.9964, val loss: 0.102003, val acc: 0.979167\n",
      "Done\n",
      "CPU times: user 44min 14s, sys: 6.18 s, total: 44min 20s\n",
      "Wall time: 44min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "EPOCHS = 6\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    correct = 0\n",
    "    #total = 0\n",
    "    train_loss , val_loss = 0.0 , 0.0\n",
    "    train_acc, val_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    model.train()\n",
    "    for data in trainloader:\n",
    "        n += 1\n",
    "        tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward \n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        # outputs ： \"(loss), logits, (hidden_states), (attentions)\"\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #get prediction and calulate acc\n",
    "        logits = outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        train_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "\n",
    "        #  batch loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    #validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in valloader:\n",
    "            m += 1\n",
    "            tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "            val_outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "            \n",
    "            logits = val_outputs[1]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            val_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "            val_loss += val_outputs[0].item()\n",
    "\n",
    "    print('[epoch %d] loss: %.4f, acc: %.4f, val loss: %4f, val acc: %4f' %\n",
    "          (epoch+1, train_loss/n, train_acc/n, val_loss/m,  val_acc/m  ))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv(data_file, encoding='UTF-8',usecols=['text'])\n",
    "test_data = f['text'].tolist()[:5000]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7827cb02966332a8decf66dd91e7ff428aabf64ecdf2401e3fd86825d4119eb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('hongchibao': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
